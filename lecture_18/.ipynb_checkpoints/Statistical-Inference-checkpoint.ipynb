{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Statistical Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Tutorial Objectives\n",
    "\n",
    "In this tutorial, we explain how to do inference through an example.\n",
    "\n",
    "By completing the exercises in this tutorial, you should:\n",
    "* understand what the likelihood function is, and have some intuition of why it is important\n",
    "* know how to summarise the Gaussian distribution using mean and variance\n",
    "* know how to maximise a likelihood function\n",
    "* be able to do simple inference in the classical way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "from numpy.random import default_rng  # a default random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Figure settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "import ipywidgets as widgets  # interactive display\n",
    "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label, interact_manual\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_hist(data, xlabel, figtitle = None, num_bins = None):\n",
    "  \"\"\" Plot the given data as a histogram.\n",
    "\n",
    "    Args:\n",
    "      data (ndarray): array with data to plot as histogram\n",
    "      xlabel (str): label of x-axis\n",
    "      figtitle (str): title of histogram plot (default is no title)\n",
    "      num_bins (int): number of bins for histogram (default is 10)\n",
    "\n",
    "    Returns:\n",
    "      count (ndarray): number of samples in each histogram bin\n",
    "      bins (ndarray): center of each histogram bin\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel('Count')\n",
    "  if num_bins is not None:\n",
    "    count, bins, _ = plt.hist(data, max(data), bins=num_bins)\n",
    "  else:\n",
    "    count, bins, _ = plt.hist(data, max(data))  # 10 bins default\n",
    "  if figtitle is not None:\n",
    "    fig.suptitle(figtitle, size=16)\n",
    "  plt.show()\n",
    "  return count, bins\n",
    "\n",
    "\n",
    "def plot_gaussian_samples_true(samples, xspace, mu, sigma, xlabel, ylabel):\n",
    "  \"\"\" Plot a histogram of the data samples on the same plot as the gaussian\n",
    "  distribution specified by the give mu and sigma values.\n",
    "\n",
    "    Args:\n",
    "      samples (ndarray): data samples for gaussian distribution\n",
    "      xspace (ndarray): x values to sample from normal distribution\n",
    "      mu (scalar): mean parameter of normal distribution\n",
    "      sigma (scalar): variance parameter of normal distribution\n",
    "      xlabel (str): the label of the x-axis of the histogram\n",
    "      ylabel (str): the label of the y-axis of the histogram\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel(xlabel)\n",
    "  ax.set_ylabel(ylabel)\n",
    "  # num_samples = samples.shape[0]\n",
    "\n",
    "  count, bins, _ = plt.hist(samples, density=True)  # probability density function\n",
    "\n",
    "  plt.plot(xspace, norm.pdf(xspace, mu, sigma), 'r-')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_likelihoods(likelihoods, mean_vals, variance_vals):\n",
    "  \"\"\" Plot the likelihood values on a heatmap plot where the x and y axes match\n",
    "  the mean and variance parameter values the likelihoods were computed for.\n",
    "\n",
    "    Args:\n",
    "      likelihoods (ndarray): array of computed likelihood values\n",
    "      mean_vals (ndarray): array of mean parameter values for which the\n",
    "                            likelihood was computed\n",
    "      variance_vals (ndarray): array of variance parameter values for which the\n",
    "                            likelihood was computed\n",
    "\n",
    "    Returns:\n",
    "      Nothing.\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(likelihoods)\n",
    "\n",
    "  cbar = ax.figure.colorbar(im, ax=ax)\n",
    "  cbar.ax.set_ylabel('log likelihood', rotation=-90, va=\"bottom\")\n",
    "\n",
    "  ax.set_xticks(np.arange(len(mean_vals)))\n",
    "  ax.set_yticks(np.arange(len(variance_vals)))\n",
    "  ax.set_xticklabels(mean_vals)\n",
    "  ax.set_yticklabels(variance_vals)\n",
    "  ax.set_xlabel('Mean')\n",
    "  ax.set_ylabel('Variance')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Statistical inference and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "After we observe $n$ data points, we can estimate the parameters of our statistical model by calculating the **likelihood** of our probability distribution model having generated each of those data points $x_i$.\n",
    "\\begin{equation}\n",
    "P(x_i|\\mu,\\sigma)=\\mathcal{N}(x_i,\\mu,\\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "For all data points $\\mathbf{x}=(x_1, x_2, x_3, ...x_n) $ we can calculate the likelihood for the whole dataset by computing the product of the likelihood for each single data point.\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mathbf{x}|\\mu,\\sigma)=\\prod_{i=1}^n \\mathcal{N}(x_i,\\mu,\\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "</details>\n",
    "\n",
    "While the likelihood may be written as a conditional probability ($P(x|\\mu,\\sigma)$), we refer to it as the **likelihood function**, $L(\\mu,\\sigma)$.  This slight switch in notation is to emphasize our focus: we use likelihood functions when the data points $\\mathbf{x}$ are fixed and we are focused on the parameters.\n",
    "\n",
    "Our new notation makes clear that the likelihood $L(\\mu,\\sigma)$ is a function of $\\mu$ and $\\sigma$, not of $\\mathbf{x}$.\n",
    "\n",
    "If we do not know the parameters $\\mu$, $\\sigma$ that generated the data, we can try to **infer** which parameter values (given our probability distribution model) gives the best (highest) likelihood. This is what we call statistical inference: trying to infer what parameters make our observed data the most likely or probable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Computing likelihood\n",
    "\n",
    "Let's start with computing the likelihood of some set of data points being drawn from a Gaussian distribution with a mean and variance we choose.\n",
    "\n",
    "\n",
    "\n",
    "As multiplying small probabilities together can lead to very small numbers, it is often convenient to report the *logarithm* of the likelihood. This is just a convenient transformation and as logarithm is a monotonically increasing function this does not change what parameters maximise the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood_normal(x, mean_val, standard_dev_val):\n",
    "  \"\"\" Computes the log-likelihood values given a observed data sample x, and\n",
    "  potential mean and variance values for a normal distribution\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): 1-D array with all the observed data\n",
    "      mean_val (scalar): value of mean for which to compute likelihood\n",
    "      standard_dev_val (scalar): value of variance for which to compute likelihood\n",
    "\n",
    "    Returns:\n",
    "      likelihood (scalar): value of likelihood for this combination of means/variances\n",
    "  \"\"\"\n",
    "\n",
    " \n",
    "\n",
    "  # Get probability of each data point (use norm.pdf from scipy stats)\n",
    "  p_data = ...\n",
    "\n",
    "  # Compute likelihood (sum over the log of the probabilities)\n",
    "  likelihood = ...\n",
    "\n",
    "  return likelihood\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "# Compute likelihood for a guessed mean/standard dev\n",
    "guess_mean = 4\n",
    "guess_standard_dev = .1\n",
    "likelihood = compute_likelihood_normal(x, guess_mean, guess_standard_dev)\n",
    "print(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You should get a likelihood of `-92904.81`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is somewhat meaningless to us! For it to be useful, we need to compare it to the likelihoods computed using other guesses of the mean or standard deviation. The visualization below shows us the likelihood for various values of the mean and the standard deviation. Essentially, we are performing a rough grid-search over means and standard deviations.  What would you guess as the true mean and standard deviation based on this visualization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute to visualize likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to visualize likelihoods\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "\n",
    "# Compute likelihood for different mean/variance values\n",
    "mean_vals = np.linspace(1, 10, 10) # potential mean values to ry\n",
    "standard_dev_vals = np.array([0.7, 0.8, 0.9, 1, 1.2, 1.5, 2, 3, 4, 5]) # potential variance values to try\n",
    "\n",
    "# Initialise likelihood collection array\n",
    "likelihood = np.zeros((mean_vals.shape[0], standard_dev_vals.shape[0]))\n",
    "\n",
    "# Compute the likelihood for observing the gvien data x assuming\n",
    "# each combination of mean and variance values\n",
    "for idxMean in range(mean_vals.shape[0]):\n",
    "  for idxVar in range(standard_dev_vals .shape[0]):\n",
    "    likelihood[idxVar,idxMean]= sum(np.log(norm.pdf(x, mean_vals[idxMean],\n",
    "                                              standard_dev_vals[idxVar])))\n",
    "\n",
    "# Uncomment once you've generated the samples and compute likelihoods\n",
    "xspace = np.linspace(0, 10, 100)\n",
    "plot_likelihoods(likelihood, mean_vals, standard_dev_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "Implicitly, by looking for the parameters that give the highest likelihood in the last section, we have been searching for the **maximum likelihood** estimate.\n",
    "\\begin{equation}\n",
    "(\\hat{\\mu},\\hat{\\sigma}) = \\underset{\\mu,\\sigma}{\\operatorname{argmax}}L(\\mu,\\sigma) = \\underset{\\mu,\\sigma}{\\operatorname{argmax}} \\prod_{i=1}^n \\mathcal{N}(x_i,\\mu,\\sigma).\n",
    "\\end{equation}\n",
    "\n",
    "In next sections, we will look at other ways of inferring such parameter variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Searching for best parameters\n",
    "\n",
    "We want to do inference on this data set, i.e. we want to infer the parameters that most likely gave rise to the data given our model. Intuitively that means that we want as good as possible a fit between the observed data and the probability distribution function with the best inferred parameters. We can search for the best parameters manually by trying out a bunch of possible values of the parameters, computing the likelihoods, and picking the parameters that resulted in the highest likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Maximum likelihood inference\n",
    "\n",
    "Try to see how well you can fit the probability distribution to the data by using the demo sliders to control the mean and standard deviation parameters of the distribution. We will visualize the histogram of data points (in blue) and the Gaussian density curve with that mean and standard deviation (in red). Below, we print the log-likelihood.\n",
    "\n",
    "- What (approximate) values of mu and sigma result in the best fit?\n",
    "- How does the value below the plot (the log-likelihood) change with the quality of fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Make sure you execute this cell to enable the widget and fit by hand!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown Make sure you execute this cell to enable the widget and fit by hand!\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "vals = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
    "\n",
    "def plotFnc(mu,sigma):\n",
    "  loglikelihood= sum(np.log(norm.pdf(vals,mu,sigma)))\n",
    "  #calculate histogram\n",
    "\n",
    "  #prepare to plot\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_ylabel('probability')\n",
    "\n",
    "  #plot histogram\n",
    "  count, bins, ignored = plt.hist(vals,density=True)\n",
    "  x = np.linspace(0,10,100)\n",
    "\n",
    "  #plot pdf\n",
    "  plt.plot(x, norm.pdf(x,mu,sigma),'r-')\n",
    "  plt.show()\n",
    "  print(\"The log-likelihood for the selected parameters is: \" + str(loglikelihood))\n",
    "\n",
    "interact(plotFnc, mu=(0.0, 15.0, 0.1),sigma=(0.1, 5.0, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Optimization to find parameters\n",
    "\n",
    "Let's again assume that we have a data set, $\\mathbf{x}$, assumed to be generated by a normal distribution (we actually generate it ourselves in line 1, so we know how it was generated!).\n",
    "We want to maximise the likelihood of the parameters $\\mu$ and $\\sigma^2$. We can do so using a couple of tricks:\n",
    "\n",
    "*   Using a log transform will not change the maximum of the function, but will allow us to work with very small numbers that could lead to problems with machine precision.\n",
    "*   Maximising a function is the same as minimising the negative of a function, allowing us to use the minimize optimisation provided by scipy.\n",
    "\n",
    "The optimisation will be done using `sp.optimize.minimize`, which does a version of gradient descent (there are hundreds of ways to do numerical optimisation, we will not cover these here!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#### Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the function to optimise, the negative log likelihood\n",
    "def negLogLike(theta, x):\n",
    "  \"\"\" Function for computing the negative log-likelihood given the observed data\n",
    "      and given parameter values stored in theta.\n",
    "\n",
    "      Args:\n",
    "        theta (ndarray): normal distribution parameters\n",
    "                        (mean is theta[0], standard deviation is theta[1])\n",
    "        x (ndarray): array with observed data points\n",
    "\n",
    "      Returns:\n",
    "        Calculated negative Log Likelihood value!\n",
    "  \"\"\"\n",
    "  return ...\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size=(n_samples, ))\n",
    "\n",
    "# Define bounds, var has to be positive\n",
    "bnds = ((None, None), (0, None))\n",
    "\n",
    "# Optimize with scipy!\n",
    "optimal_parameters = sp.optimize.minimize(negLogLike, (2, 2), args=x, bounds=bnds)\n",
    "print(f\"The optimal mean estimate is: {optimal_parameters.x[0]}\")\n",
    "print(f\"The optimal standard deviation estimate is: {optimal_parameters.x[1]}\")\n",
    "\n",
    "# optimal_parameters contains a lot of information about the optimization,\n",
    "# but we mostly want the mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These are the approximations of the parameters that maximise the likelihood ($\\mu$ ~ 5.280 and $\\sigma$ ~ 1.148).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Analytical solution\n",
    "\n",
    "Sometimes, things work out well and we can come up with formulas for the maximum likelihood estimates of parameters. We won't get into this further but basically we could set the derivative of the likelihood to 0 (to find a maximum) and solve for the parameters. This won't always work but for the Gaussian distribution, it does.\n",
    "\n",
    "Specifically , the special thing about the Gaussian is that mean and standard deviation of the random sample can effectively approximate the two parameters of a Gaussian, $\\mu, \\sigma$.\n",
    "\n",
    "\n",
    "Hence using the  mean, $\\bar{x}=\\frac{1}{n}\\sum_i x_i$, and variance, $\\bar{\\sigma}^2=\\frac{1}{n} \\sum_i (x_i-\\bar{x})^2 $ of the sample should give us the best/maximum likelihood, $L(\\bar{x},\\bar{\\sigma}^2)$.\n",
    "\n",
    "Let's compare these values to those we've been finding using manual search and optimization, and the true values (which we only know because we generated the numbers!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "true_mean = 5\n",
    "true_standard_dev = 1\n",
    "n_samples = 1000\n",
    "x = np.random.normal(true_mean, true_standard_dev, size=(n_samples, ))\n",
    "\n",
    "# Compute and print sample means and standard deviations\n",
    "print(f\"This is the sample mean as estimated by numpy: {np.mean(x)}\")\n",
    "print(f\"This is the sample standard deviation as estimated by numpy: {np.std(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "If you try out different values of the mean and standard deviation in all the previous exercises, you should see that changing the mean and\n",
    "sigma parameter values (and generating new data from a distribution with these parameters) makes no difference as MLE methods can still recover these parameters.\n",
    "\n",
    "There is a slight problem: it turns out that the maximum likelihood estimate for the variance is actually a biased one! This means that the estimators expected value (mean value) and the true value of the parameter are different.  An unbiased estimator for the variance is $\\bar{\\sigma}^2=\\frac{1}{n-1} \\sum_i (x_i-\\bar{x})^2 $, this is called the sample variance. For more details, see [the wiki page on bias of estimators](https://en.wikipedia.org/wiki/Bias_of_an_estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "Having done the different exercises you should now:\n",
    "* understand what the likelihood function is, and have some intuition of why it is important\n",
    "* know how to summarise the Gaussian distribution using mean and variance\n",
    "* know how to maximise a likelihood function\n",
    "* be able to do simple inference in the classical way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this notebook\n",
    "\n",
    "Most of the content here is directly adapted from Neuromatch Academy's Statistics Tutorial"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W0D5_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
